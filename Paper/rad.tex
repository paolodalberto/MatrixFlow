%% This is file `sample-manuscript.tex', generated with the docstrip
%% utility.  The original source files were: samples.dtx (with
%% options: `manuscript') IMPORTANT NOTICE: For the copyright see the
%% source file.  Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.  For
%% distribution of the original source see the terms for copying and
%% modification in the file samples.dtx.  This generated file may be
%% distributed as long as the original source files, as listed above,
%% are part of the same distribution. (The sources need not
%% necessarily be in the same archive or directory.)  The first
%% command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall]{acmart}
%\usepackage{bm}

%% as manuscript for review this will create line number 
\usepackage{lineno}
%\linenumbers

%%  Always include hyperref last
%% \usepackage[bookmarks=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=black,citecolor=blue,urlcolor=black]{hyperref}

% Paolo's definition. Yes, I have my way to define stuff. Please take
% a look
\input{mydef}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{}

%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{--}
\acmNumber{--}
\acmArticle{---}
\acmMonth{3}


%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Strassen's Matrix Multiplication Algorithm Is Still Faster}

\author{Paolo D'Alberto}
\email{}
\affiliation{%
  \institution{AMD}
  \streetaddress{2100 Logic Dr}
  \city{San Jose}
  \state{California}
  \postcode{95124}
}

\renewcommand{\shortauthors}{D'Alberto et al.}

\begin{abstract}
  Recently, reinforcement algorithms discovered new algorithms that
  may replace Strassen's original fast matrix multiplication. Likely,
  there are even more hidden gems by way of factoring the operand
  matrices. The discovery really jump-started a wave of excitement and
  a flourishing of publications. However, little work on
  implementations and applications.

  We present {\em Matrix Flow}, this is a simple Python project for
  the automatic formulation, design, implementation, code generation,
  and execution of fast matrix multiplication algorithms for CPUs and
  GPUs. We play with no module-2 algorithms, because even less
  general, and for square matrices, for simplicity.

  We have an original way to combine fast algorithms by means of
  factorizing the operand matrices and prove them correct. We describe
  these algorithm as a Data flow and matrix data partitioning: a
  Directed Acyclic Graph. We show that Strassen's original algorithm
  is still the top choice even for modern GPUs. We also address error
  analysis in double precision, because integer computations are
  correct, always.
\end{abstract}

\maketitle

\section{Introduction} 
\label{sec:introduction}
In practice, there are quite a few algorithms for {\em fast} matrix
multiplication. We used to have a few pages of references; now
institutions are actually stepping in and providing a complete
list. Although the list of algorithms is long, the list of
implementation for old and new architectures is limited (if not
existent). We are interested in the computation with matrices, not
just a single element, and we compare the trade matrix multiplication
for pre and post matrix additions. This could be important when we
tray to translate fast algorithms into practical implementations.


The researchers at DeepMind presented a new methodology to find new
and faster matrix multiplications \cite{PMID:36198780}. They provide
the algorithms in matrix forms and, easily, we can generate, and
execute element-wise algorithms.  There are new algorithms and They
show the relative advantage performance with respect to the classic
matrix multiplication for GPUs and TPUs. 

Here, we show that Strassen's algorithm \cite{STRASSEN1969} is still
king of the hill.  We explore a comparison across several algorithms
using the standard arithmetic and in particular double precision
matrices (no $\Z_2$ modular arithmetic because Strassen's algorithm
does not have equivalent). In this scenario, we can determine a priori
the relative advantages of all algorithms (7\% improvement per
recursion we shall expand). More interesting, we can show that {\em
  deeper} algorithms such as the one recently discovered do not have
all the advantages (they are cranked up to have) when other
constraints are present.

In the following, we present a python project that we call {\em Matrix
  Flow}. Here, you can take any of the algorithms for square matrices,
we use the DeepMind's algorithms as repository, and create complex
implementations you can run in CPU, GPU and in principle extend to any
accelerator using the appropriate interface. 

We have a few goals in mind for this work. First, we want to make
available algorithms and Python implementations: both proven
correct. Second, we want to provide tools for the implementation of
these algorithms for highly efficient systems. Python is flexible,
with such a nice abstraction level that writing code for matrix
algorithms is a breeze. Efficiency and performance is not always
Python forte, however writing interfaces for C/C++ is relatively
simple. Interfaces are not a very good solution either and they are
more a nice intermediary solution. So we provide tools to build entire
algorithms in C/C++ for CPU and GPUs and still prove them correct and
validate them easily.

So this is a research paper and a software project
introduction. Eventually, we would like any one to play with the
project and we would like to squeeze into the presentation and share a
few lessons learned that are not trivial, sometime original, and
currently important for other applications where compiler tools are
designed to optimize artificial intelligence applications.


\section{Matrices and Partitions.}
We start by describing the basic components of our algebra: Matrices,
operations, and sometimes underrated Partition of Matrices. This will
help understand fast matrix multiplications constructively.

In this work we work mostly with matrices and scalar, but vector are
easily added. A matrix $\Vc{A}$, this is a square matrix of size
$\R^{n\times n}$. Every element of the matrix is identified by a row
$i$ and a column $j$ as $\Vc{A}_{i,j}$. A scalar $\alpha$ is number in
$\R$.
\begin{itemize}
  \item $\Vc{B} = \alpha\Vc{A} = \Vc{A}\alpha$ so that $\Vc{B}_{i,j}
    = \alpha\Vc{A}_{i,j}$ and the real multiplication.
  \item $\Vc{B} = \alpha\Vc{B} + \beta\Vc{C}= \beta\Vc{C} +
    \alpha\Vc{B}$ is matrix addition and $\Vc{B}_{i,j} =
    \alpha\Vc{B}_{i,j} + \beta\Vc{C}_{i,j}$
  \item $\Vc{B} = \alpha(\Vc{B} + \Vc{C}) = \alpha\Vc{B} +
    \alpha\Vc{C}$
  \item $\Vc{C}= \Vc{A}\Vc{B}$ is matrix multiplication and $\Vc{C}_{i,j}=$ 
    $\sum_{k=1}^n \Vc{A}_{i,k}\Vc{B}_{k,j}$
  \item $\Vc{y}= \alpha\Vc{A}\Vc{x}$ is matrix by vector
    multiplication and $\Vc{y}_{i}=\alpha \sum_{k=1}^n
    \Vc{A}_{i,k}\Vc{b}_{k}$ (you may find this in the software
    package).
\end{itemize}

Great, we have the operands and the operations. Let us introduce the
last component for the description of our algorithms: Partitions.  The
double subscript for the definition of elements it is for exposition
purpose and we do not use element operation any where. We introduce
here the single subscript that we use a lot. Consider a matrix $\Vc{A}
\in \R^{n\times n}$, we can consider the matrix as a composition of
$\Vc{A}_i \in \R^{\frac{n}{2}\times \frac{n}{2}}$ sub-matrices.
\begin{equation} 
  \Vc{A} =
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 \\
    \Vc{A}_2 & \Vc{A}_3 \\
  \end{pmatrix} = \{ \Vc{A}_0 , \Vc{A}_1, \Vc{A}_2 , \Vc{A}_3  \}
\end{equation}
The property is that $\Vc{A}_i$ are non overlapping matrices and this
is easily generalized to any factor of $n$ for example $p$ so that $n
= k*p$. For $n=p=3$ and $\Vc{A}_i \in \R^{\frac{n}{3}\times
  \frac{n}{3}}$ and $0\leq i\leq p^2 -1$:
\begin{equation} 
  \Vc{A} =
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 & \Vc{A}_2 \\
    \Vc{A}_3 & \Vc{A}_4 & \Vc{A}_{5} \\
    \Vc{A}_6 & \Vc{A}_7 & \Vc{A}_8 \\
  \end{pmatrix}\\
  = \{ \Vc{A}_0 , \Vc{A}_1, \Vc{A}_2 , \Vc{A}_3 ,\Vc{A}_4 , \Vc{A}_5, \Vc{A}_6 , \Vc{A}_7, \Vc{A}_8   \}
\end{equation}

In practice, if we have a partition $\{ \Vc{A}_i \}$ and we know the
factor $p$, we know the matrix $\Vc{A}$ completely.

Let us represent the simplest matrix multiplication using $2x2$ partition:

\begin{equation} 
  \begin{pmatrix}
    \Vc{C}_0 & \Vc{C}_1 \\
    \Vc{C}_2 & \Vc{C}_3 \\
  \end{pmatrix} =
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 \\
    \Vc{A}_2 & \Vc{A}_3 \\
  \end{pmatrix} *
  \begin{pmatrix}
    \Vc{B}_0 & \Vc{B}_1 \\
    \Vc{B}_2 & \Vc{B}_3 \\
  \end{pmatrix} 
\end{equation}
Using the factor $p=2$, we have in general
\begin{equation}
  \label{eq:recusion}
  \begin{pmatrix} 
    \Vc{C}_0 = \sum_{k=0}^{1}\Vc{A}_k    \Vc{B}_{2k} &
    \Vc{C}_1 = \sum_{k=0}^{1}\Vc{A}_k    \Vc{B}_{2k+1} \\
    \Vc{C}_2 = \sum_{k=0}^{1}\Vc{A}_{2+k} \Vc{B}_{2k} &
    \Vc{C}_3 = \sum_{k=0}^{1}\Vc{A}_{2+k} \Vc{B}_{2k+1} \\
  \end{pmatrix}
\end{equation}
The single subscript is the original format in fast algorithms
describing a matrix operand as partitioned into smaller matrices. The
authors in \cite{PMID:36198780} use a double subscript because the
intend to identify a single element. Strangely enough, the single
subscript has an implementation advantage.

Equation \ref{eq:recusion}, as matrix computation, represent naturally
a recursive computation 











%\input{conclusion.tex}

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{ACM-Reference-Format} \bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix{Review and Response}
%\input{review.tex}
\end{document}




  


  




  


