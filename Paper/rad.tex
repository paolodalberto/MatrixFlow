%% This is file `sample-manuscript.tex', generated with the docstrip
%% utility.  The original source files were: samples.dtx (with
%% options: `manuscript') IMPORTANT NOTICE: For the copyright see the
%% source file.  Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.  For
%% distribution of the original source see the terms for copying and
%% modification in the file samples.dtx.  This generated file may be
%% distributed as long as the original source files, as listed above,
%% are part of the same distribution. (The sources need not
%% necessarily be in the same archive or directory.)  The first
%% command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall]{acmart}
%\usepackage{bm}

%% as manuscript for review this will create line number 
\usepackage{lineno}
%\linenumbers

%%  Always include hyperref last
%% \usepackage[bookmarks=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=black,citecolor=blue,urlcolor=black]{hyperref}

% Paolo's definition. Yes, I have my way to define stuff. Please take
% a look
\input{mydef}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{}

%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{--}
\acmNumber{--}
\acmArticle{---}
\acmMonth{3}


%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Strassen's Matrix Multiplication Algorithm Is Still Faster}

\author{Paolo D'Alberto}
\email{}
\affiliation{%
  \institution{AMD}
  \streetaddress{2100 Logic Dr}
  \city{San Jose}
  \state{California}
  \postcode{95124}
}

\renewcommand{\shortauthors}{D'Alberto et al.}

\begin{abstract}
  Recently, reinforcement algorithms discovered new algorithms that
  may replace Strassen's original fast matrix multiplication. Likely,
  there are even more hidden gems by way of factoring the operand
  matrices. The discovery really jump-started a wave of excitement and
  a flourishing of publications. However, little work on
  implementations and applications.

  We present {\em Matrix Flow}, this is a simple Python project for
  the automatic formulation, design, implementation, code generation,
  and execution of fast matrix multiplication algorithms for CPUs and
  GPUs. We play with no module-2 algorithms, because even less
  general, and for square matrices, for simplicity.

  We have an original way to combine fast algorithms by means of
  factorizing the operand matrices and prove them correct. We describe
  these algorithm as a Data flow and matrix data partitioning: a
  Directed Acyclic Graph. We show that Strassen's original algorithm
  is still the top choice even for modern GPUs. We also address error
  analysis in double precision, because integer computations are
  correct, always.
\end{abstract}

\maketitle

\section{Introduction} 
\label{sec:introduction}
In practice, there are quite a few algorithms for {\em fast} matrix
multiplication. We used to have a few pages of references; now
institutions are actually stepping in and providing a complete
list. Although the list of algorithms is long, the list of
implementation for old and new architectures is limited (if not
existent). We are interested in the computation with matrices, not
just a single element, and we compare the trade matrix multiplication
for pre and post matrix additions. This could be important when we
tray to translate fast algorithms into practical implementations.


The researchers at DeepMind presented a new methodology to find new
and faster matrix multiplications \cite{PMID:36198780}. They provide
the algorithms in matrix forms and, easily, we can generate, and
execute element-wise algorithms.  There are new algorithms and They
show the relative advantage performance with respect to the classic
matrix multiplication for GPUs and TPUs. 

Here, we show that Strassen's algorithm \cite{STRASSEN1969} is still
king of the hill.  We explore a comparison across several algorithms
using the standard arithmetic and in particular double precision
matrices (no $\Z_2$ modular arithmetic because Strassen's algorithm
does not have equivalent). In this scenario, we can determine a priori
the relative advantages of all algorithms (7\% improvement per
recursion we shall expand). More interesting, we can show that {\em
  deeper} algorithms such as the one recently discovered do not have
all the advantages (they are cranked up to have) when other
constraints are present.

In the following, we present a python project that we call {\em Matrix
  Flow}. Here, you can take any of the algorithms for square matrices,
we use the DeepMind's algorithms as repository, and create complex
implementations you can run in CPU, GPU and in principle extend to any
accelerator using the appropriate interface. 

We have a few goals in mind for this work. First, we want to make
available algorithms and Python implementations: both proven
correct. Second, we want to provide tools for the implementation of
these algorithms for highly efficient systems. Python is flexible,
with such a nice abstraction level that writing code for matrix
algorithms is a breeze. Efficiency and performance is not always
Python forte, however writing interfaces for C/C++ is relatively
simple. Interfaces are not a very good solution either and they are
more a nice intermediary solution. So we provide tools to build entire
algorithms in C/C++ for CPU and GPUs and still prove them correct and
validate them easily.

So this is a research paper and a software project
introduction. Eventually, we would like any one to play with the
project and we would like to squeeze into the presentation and share a
few lessons learned that are not trivial, sometime original, and
currently important for other applications where compiler tools are
designed to optimize artificial intelligence applications.


\section{Matrices and Partitions.}
We start by describing the basic components of our algebra: matrices,
operations, and matrix partitions . This will help us understand and
describe fast matrix multiplications constructively.

In this work we work mostly with matrices and scalar, but vector are
easily added. A matrix $\Vc{A}$, this is a square matrix of size
$\R^{n\times n}$. Every element of the matrix is identified by a row
$i$ and a column $j$ as $\Vc{A}_{i,j}$. A scalar $\alpha$ is number in
$\R$.
\begin{itemize}
  \item $\Vc{B} = \alpha\Vc{A} = \Vc{A}\alpha$ so that $\Vc{B}_{i,j}
    = \alpha\Vc{A}_{i,j}$ and the real multiplication.
  \item $\Vc{B} = \alpha\Vc{B} + \beta\Vc{C}= \beta\Vc{C} +
    \alpha\Vc{B}$ is matrix addition and $\Vc{B}_{i,j} =
    \alpha\Vc{B}_{i,j} + \beta\Vc{C}_{i,j}$
  \item $\Vc{B} = \alpha(\Vc{B} + \Vc{C}) = \alpha\Vc{B} +
    \alpha\Vc{C}$
  \item $\Vc{C}= \Vc{A}\Vc{B}$ is matrix multiplication and $\Vc{C}_{i,j}=$ 
    $\sum_{k=1}^n \Vc{A}_{i,k}\Vc{B}_{k,j}$
  \item $\Vc{y}= \alpha\Vc{A}\Vc{x}$ is matrix by vector
    multiplication and $\Vc{y}_{i}=\alpha \sum_{k=1}^n
    \Vc{A}_{i,k}\Vc{b}_{k}$ (you may find this in the software
    package).
\end{itemize}

Great, we have the operands and the operations. Let us introduce the
last component for the description of our algorithms: Partitions.  The
double subscript for the definition of elements it is for exposition
purpose and we do not use element operation any where. We introduce
here the single subscript that we use a lot. Consider a matrix $\Vc{A}
\in \R^{n\times n}$, we can consider the matrix as a composition of
$\Vc{A}_i \in \R^{\frac{n}{2}\times \frac{n}{2}}$ sub-matrices.
\begin{equation} 
  \Vc{A} =
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 \\
    \Vc{A}_2 & \Vc{A}_3 \\
  \end{pmatrix} = \{ \Vc{A}_0 , \Vc{A}_1, \Vc{A}_2 , \Vc{A}_3  \}
\end{equation}
The property is that $\Vc{A}_i$ are non overlapping matrices and this
is easily generalized to any factor of $n$ for example $p$ so that $n
= k*p$. For $n=p=3$ and $\Vc{A}_i \in \R^{\frac{n}{3}\times
  \frac{n}{3}}$ and $0\leq i\leq p^2 -1$:
\begin{equation} 
  \Vc{A} =
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 & \Vc{A}_2 \\
    \Vc{A}_3 & \Vc{A}_4 & \Vc{A}_{5} \\
    \Vc{A}_6 & \Vc{A}_7 & \Vc{A}_8 \\
  \end{pmatrix}\\
  = \{ \Vc{A}_0 , \Vc{A}_1, \Vc{A}_2 , \Vc{A}_3 ,\Vc{A}_4 , \Vc{A}_5, \Vc{A}_6 , \Vc{A}_7, \Vc{A}_8   \}
\end{equation}

In practice, if we have a partition $\{ \Vc{A}_i \}$ and we know the
factor $p$, we know the matrix $\Vc{A}$ completely.

Let us represent the simplest matrix multiplication using $2x2$ partition:

\begin{equation}
  \label{basic}
  \begin{pmatrix}
    \Vc{C}_0 & \Vc{C}_1 \\
    \Vc{C}_2 & \Vc{C}_3 \\
  \end{pmatrix} =
  \begin{pmatrix}
    \Vc{A}_0 & \Vc{A}_1 \\
    \Vc{A}_2 & \Vc{A}_3 \\
  \end{pmatrix} *
  \begin{pmatrix}
    \Vc{B}_0 & \Vc{B}_1 \\
    \Vc{B}_2 & \Vc{B}_3 \\
  \end{pmatrix} 
\end{equation}
Using the factor $p=2$, we have in general
\begin{equation}
  \label{eq:recursion}
  \begin{pmatrix} 
    \Vc{C}_0 = \sum_{k=0}^{1}\Vc{A}_k    \Vc{B}_{2k} &
    \Vc{C}_1 = \sum_{k=0}^{1}\Vc{A}_k    \Vc{B}_{2k+1} \\
    \Vc{C}_2 = \sum_{k=0}^{1}\Vc{A}_{2+k} \Vc{B}_{2k} &
    \Vc{C}_3 = \sum_{k=0}^{1}\Vc{A}_{2+k} \Vc{B}_{2k+1} \\
  \end{pmatrix}
\end{equation}
The single subscript is the original format in fast algorithms
describing a matrix operand as partitioned into smaller matrices. The
authors in \cite{PMID:36198780} use a double subscript because the
intend to identify a single element. Strangely enough, the single
subscript has an implementation advantage.

Equation \ref{eq:recursion}, as matrix computation, represent naturally
a recursive computation.

A Strassen's algorithm or any fast algorithm has the following format:
\begin{equation}
  \label{strassen}
C_i = \sum_{j=0}^6 c_{i,j}P_j =\sum_{j=0}^6 c_{i,j}T_j*S_j  
=\sum_{j=0}^6 c_{i,j}\Big[ T_j = \big(\sum_{k=0}^3
  a_{k,j}A_k\big)\Big]* \Big[S_j = \big(\sum_{\ell=0}^3 b_{\ell,j}B_\ell\big) \Big]
\end{equation}
There are seven products, each product can be done recursively, but
first we must combine submatrices of the operands accordingly to a
specific set of coefficients. The coefficients $c_{i,j},a_{k,j},$ and
$b_{\ell,j}$ belong to three sparse matrices. Let us take the
Strassen's algorithm but represented as DeepMind would use it for the
computation of Equation \ref{strassen}. We choose to use single
subscripts for $C_i, A_j, B_k$, because the notation is simpler (i.e.,
fewer indices), and because we implicitly use a row major layout of
the sub-matrices making the single index completely determined.

\begin{equation} 
  \Vc{a}, \Vc{b}, \Vc{c}^t = f[`2,2,2`] \\
\end{equation}
Where the matrices $\Vc{a}, \Vc{b}$, and $\Vc{c}$ are integer matrices
with coefficients [-1,0,1], but they do not need to be integers,
described as follow plus some useful notations:

\begin{equation}
  \Vc{a}= 
  \begin{pmatrix}
    A_0 & 0   & 1  & 1 &  0 &  1&  1&  0 \\
    A_1 & 0   & 0  &-1 &  1 &  0&  0&  0 \\ 
    A_2 & 1   & 1  & 1 &  0 &  1&  0&  0 \\
    A_3 &-1   &-1  &-1 &  0 &  0&  0&  1 \\ \hline
        & T_0 &T_1 &T_2 &T_3&T_4 &T_5&T_6 \\
  \end{pmatrix},
  \Vc{b}= 
  \begin{pmatrix}
    B_0 &0   & 0 & 0 & 0 & 1 & 1 & 0 \\
    B_1 &1   & 1 & 0 & 0 & 1 & 0 & 1 \\
    B_2 &0   & 1 & 1 & 1 & 1 & 0 & 0 \\
    B_3 &0   & 1 & 1 & 0 & 1 & 0 & 1 \\ \hline
        &S_0 &S_1&S_2&S_3 &S_4&S_5&S_6 \\
  \end{pmatrix},
\end{equation}
and

\begin{equation}
  \Vc{c}^t =
  \begin{pmatrix}
    C_0 &  0&  0&  0&  1&  0&  1&  0 \\
    C_2 &  0& -1&  0&  0&  1& -1& -1 \\
    C_1 & -1&  1& -1& -1&  0&  0&  0 \\
    C_3 &  1&  0&  0&  0&  0&  0&  1 \\ \hline
        &P_0&P_1&P_2&P_3 &P_4&P_5&P_6 \\
  \end{pmatrix} 
\end{equation}

Take matrix $\Vc{c}$ and consider the first row ($C_0$).
\begin{equation}
  \Vc{C}_0 = P_3+P_5 = (T_3*S_3) + (T_5*S_5) = (A_1)*(B_3) + (A_0)*(B_0)
\end{equation}
and for better effect the last row ($C_3$).
\begin{equation}
  \Vc{C}_3 = P_0+P_6 = (T_0*S_0) + (T_6*S_6) = (A_2-A_1)*(B_1) + (A_3)*(B_1+B_3)
\end{equation}
If we take the matrix $\Vc{c}^t$, the columns are the products of the
algorithms, in this case 7. The row explains how to sum these product
so that to achieve the correct result. The matrix $\Vc{a}$, each
column $i$ represents the left operand of product $P_i$. The matrix
$\Vc{b}$ represent the right operand. Notice, that with the proper
matrices we could achieve Equation \ref{eq:recursion} 




We shall use only square matrices for simplicity. Thus the key for the
full determination of the Strassen algorithm is 2. This is the common
factor we use to split both sides (i.e., row and column) for all
matrices. If the split factor determines the algorithm with minimum
number of products, so we are going to {\em play} with 2 as 7
products, 3 as 23 products, 4 as 2x2 and 49 products, 6 = 2x3 and 3x2
as 7*23 products, 9 = 3x3 as 23*23 products, 12=2x2x3, 2x3x2, 3x2x2 as
7*7*23 products.

Intuitively and if we are using the recursive idea as in Equation
\ref{strassen}, take the algorithm with factor 6. We can take fist the
factor 2, and each of the 7 product, recursively use an algorithm with
factor 3 and 23 products. A pure recursive algorithm would compute the
operands $T_i$ and $S_i$ first, recursively solve the result $P_i$ and
distribute it. The sequence 2 and 3 specifies an algorithm that is
different from the sequence 3 and 2.  The space requirement is
different: literally if we take a 6x6 matrix, if we split the matrices
by 2, the temporary space to hold $T_i$ and $S_i$ are two matrices of
size 3x3. The recursive call will use temporary space of size
1x1. Ohterwise, if we split by 3, we need a temporary space of size
2x2. The computation order is different and (in double precision)
there is a computational difference.

Interestingly, the original problem (6x6x6), which is the complete
reference in \cite{PMID:36198780} splits the problem $(M,N,K)$ into
$(\frac{M}{6},\frac{N}{6}\frac{K}{6})$ basic products can be done
using non square ones: (2x3x3) and (3x2x3). Thus, we can solve
directly using a factor 6 and square matrices but with a proper
factorization of $\Vc{a},\Vc{b}$, and $\Vc{c}$ and the order is
important.

In the project there is a complete proof and a few implementations: In
practice, if we use a factor 3 and we have $\Vc{a}_3, \Vc{b}_3,
\Vc{c}_3$ and we have a factor 2 with $\Vc{a}_2, \Vc{b}_2, \Vc{c}_2$,
then the algorithm with factor 6 is completely defined succinctly as
$\Vc{a}_3 \otimes \Vc{a}_2, \Vc{b}_3 \otimes \Vc{b}_2, \Vc{c}_3
\otimes \Vc{c}_2$. Where $\otimes$ is the Kronecker product (used also
in \cite{PMID:36198780}).

\section{Matrix Flow}






%\input{conclusion.tex}

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{ACM-Reference-Format} \bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix{Review and Response}
%\input{review.tex}
\end{document}




  


  




  


