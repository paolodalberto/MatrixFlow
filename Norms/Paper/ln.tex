%% This is file `sample-manuscript.tex', generated with the docstrip
%% utility.  The original source files were: samples.dtx (with
%% options: `manuscript') IMPORTANT NOTICE: For the copyright see the
%% source file.  Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.  For
%% distribution of the original source see the terms for copying and
%% modification in the file samples.dtx.  This generated file may be
%% distributed as long as the original source files, as listed above,
%% are part of the same distribution. (The sources need not
%% necessarily be in the same archive or directory.)  The first
%% command in your LaTeX source must be the \documentclass command.
\documentclass[acmsmall]{acmart}
%\usepackage{bm}

%% as manuscript for review this will create line number 
\usepackage{lineno}
\usepackage{algorithm}
\usepackage{algorithmic}
%\linenumbers

%%  Always include hyperref last
%% \usepackage[bookmarks=true,breaklinks=true,letterpaper=true,colorlinks,linkcolor=black,citecolor=blue,urlcolor=black]{hyperref}

% Paolo's definition. Yes, I have my way to define stuff. Please take
% a look
\input{mydef}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2023}
\acmYear{2023}
\acmDOI{}

%% These commands are for a JOURNAL article.
\acmJournal{JACM}
\acmVolume{--}
\acmNumber{--}
\acmArticle{---}
\acmMonth{3}


%% These commands are for a PROCEEDINGS abstract or paper.
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
%  Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%%%%%%%%%%---SETME-----%%%%%%%%%%%%%
\title{Norms: A Unified Approach}

\author{Paolo D'Alberto}
\email{}
\affiliation{%
  \institution{AMD}
  \streetaddress{2100 Logic Dr}
  \city{San Jose}
  \state{California}
  \postcode{95124}
}

\renewcommand{\shortauthors}{D'Alberto et al.}

\begin{abstract}

  Layer Norms are important tools in today's AI systems.  Also, they
  are the little pesky layers that inference-engine tools have often
  problems in either fusing with other layers for speed-up or
  introducing of numerical unwanted oddities.  Norms have linear
  complexity, they are communication bound, they require to read the
  input at least twice, and there is a satisfying asymmetry of the
  computation (i.e., row Gaussian normalization and column scaling).
  
  Here, we formalize the norms using a unified formalism. We do so to
  clarify the general computation, the opportunity to fuse other
  computations, to highlight non obvious properties and give an
  unified computation model. So we can generate consistent and correct
  code for our AIE AMD computes.
  
\end{abstract}

\maketitle

\section{A few  definitions} 
\label{sec:introduction}

A matrix is $\Vc{X} \in \R^{M \times N}$ with $M$ rows and $N$ columns
and a single scalar element is $x_{i,j}$ with obvious bounds. A
vectors is $\Vc{V} \in \R^{M}$ with scalar element $v_i$.  Last, a
scalar is $\alpha \in \R$.


A matrix norm is a two-part computation. We have a {\em projection}
where we compute a factor, a vector, and then a {\em normalization}
where we take the projected factor and distribute it back to the
elements of the matrix. In the literature and implementations, these
computations have also other names: Projection is a reduction because
it is frequently associated with sums, dot products. Normalization is
a broadcast. Now that we have a names, let us define them formally and
constructively.

Let us start with scalar operations on Matrix/Vectors:
\begin{definition}
  A scalar operation on a matrix $\Vc{X}$
  \begin{equation}
    G_f: \Vc{X} \rightarrow \Vc{Y} \text{ and }  y_{i,j} = f(x_{i,j})
  \end{equation}
\end{definition}

For example, we will use $G_{x*x}(\Vc{X}) = x_{i,j}^2$ and
$G_{\exp}(\Vc{X}) = \exp(x_{i,j})$. 


\begin{definition}
An associative Projection $P_+: \Vc{X}\in\R^{M\times N} \rightarrow
\Vc{F} \in \R^{M}$, is a function where the element
\[ f_i = \sum_{j=1}^N x_{i,j}. \]
The $+/\sum$ sign is not in general a summation, We choose it because
this is a natural symbol for the associative property:
\[
f_i = (x_{i,1}) + (\sum_{j=2}^N x_{i,j}) = (\sum_{j=1}^{N-1} x_{i,j}) + (x_{i,N}).
\]

\end{definition}
The definition is a row projection.  There are norms where the
projection is by column. The formalism can easily take care of that by
specifying a direction (i.e., the code does introduce) or just
considering the transpose of the matrix.  Let us digest our formalism
a little.  As we will read our matrix, we are going to maintain the
order and projecting of matrix $\Vc{X}$ (row elements $x_{i,*}$). We
do not need the projection to be commutative in general but all the
projections of interests are. Also, we require the computation of
multiple projections for the computation of the normalization factor.

We abuse the notation of the symbol $+$ in the associative expression
$f_i = (x_{i,1} + (\sum_{j=2}^N x_{i,j}))$, in the sense that the
combination of the temporary sub-projection may require some extra
work during the computation or at the end when we have a complete
projection into a proper normalization factor. However, such
simplification does not detract any information and the implementation
can address it.

What could $P_+(G_f(X))$ be? That is, we apply scalar operations and
then projection. The associativity should be a property of the
projection and not of the value of the matrix. SoftMax Norm in finite
precision is an example where we have {\em to work} to enforce the
associativity (e.g., $\exp(x)$ can be large and in finite precision
overflow easily especially their sum).

\begin{definition}
  An optional summary function $S_+: \{\Vc{F}\}_i\in\R^{ M}
  \rightarrow \Vc{E} \in \R^{M}$ and  $\Vc{F}_i = P_i(G_{f_i}(\Vc{X}))$
\end{definition}
So we can summarize the first part of the computation by an
associative projection or a set of projections and an optional summary
function for the final composition of the normalization factors.

Now let start formulating the normalization computation.

\begin{definition}
  We have two normalization functions by row 
  \begin{equation}
    N_*: \Vc{X}\in\R^{M\times N} \times \Vc{T}\in \R^M \rightarrow \Vc{Y}
    \in \R^{M\times N}, \text{ where } y_{i,j} = x_{i,j}*t_i
  \end{equation}
  and by column
  \begin{equation}
    N_*: \Vc{X}\in\R^{M\times N} \times  \Vc{S}\in \R^N \rightarrow \Vc{Y}
    \in \R^{M\times N}, \text{ where } y_{i,j} = x_{i,j}*s_j
  \end{equation}
  Again the $*$ is not necessarily a multiplication and the
  normalization can be by row or by column. For presentation purpose
  we will use a single symbol (unless confusing).
\end{definition}

If we extend the normalizing factor $\Vc{F}$ into a diagonal matrix in
$\Vc{D}_F \in \R^{M\times M}$ so that $[\Vc{D}_F]_{i,i} = f_i$ and
zero everywhere else (same idea for $\R^{N\times N}$), and the
normalization is a based on a scalar product, then we reduce the
computation to matrix-matrix multiplication products. 
\begin{equation}
  \Vc{D}_F * \Vc{X} \text{ and } \Vc{X} * \Vc{D}_F
\end{equation}

Notice that using matrix multiplication we can see that a row
normalization is associative $(\Vc{D}_f * \Vc{X}) * \Vc{Y} = \Vc{D}_f
* (\Vc{X} * \Vc{Y})$ then giving an hint how the normalization could
be postponed if neceesary.  Also a column mormalization in combination
with a matrix multiplication $(\Vc{X}* \Vc{D}_f) * \Vc{Y} = \Vc{X}*
(\Vc{D}_f * \Vc{Y})$ is actually a row normalization of the following
operand.

We introduce the last definiition, a matrix partition.
\begin{definition}
  A row partition of a matrix is simply defined as
  \begin{equation}
    [\Vc{X} ]_{r=2} = \
    \begin{pmatrix}
      \Vc{X}_1  \\
      \Vc{X}_2  \\
    \end{pmatrix}\\
  \end{equation}
  and a column partition
  \begin{equation}
    [\Vc{X} ]_{c=2} = \
    \begin{pmatrix}
      \Vc{X}_1  & \Vc{X}_2  \\
    \end{pmatrix}\\
  \end{equation}
\end{definition}
If we want to take a matrix $\Vc{X} \in \R^{M\times N}$ and normalize
its row by a square norm see Algorithm \ref{alg:l2}. As we define
$P_+(\Vc{X})_i$ as the summation $\sum_j x_{i,j}$ and $N_*(\Vc{X},
\Vc{T})_{i,j} = x_{i,j}*t_i$, we have the complete computation of the
norm : $f_i = \sqrt{\frac{1}{N}\sum_{j=1}^N x_{i,j}^2}$ and $x_{i,j}
= \frac{x_{i,j}}{f_i}$.

\begin{algorithm}
    \caption{$\Vc{X}/||\Vc{X}||_2$ algorithm}
    \label{alg:l2}
    \begin{algorithmic}
      \STATE $\Vc{T} = P_+(G_{x*x}(\Vc{X}))$
      \STATE --------------------------------
      \STATE $\Vc{T} = \sqrt{\Vc{T}/N}$    \COMMENT{ \# Summary} 
      \STATE --------------------------------
      \STATE $\Vc{X} = N_*(\Vc{X},\Vc{T})$  
    \end{algorithmic}
\end{algorithm}

When we introduce a new projections $P_{max}(\Vc{X})_i =
\max{x_{i,*}}$, which is associative, and we normalize the additive
projection with the result of the new projection, we can see that the
Softmax computation has the same properties and composition (Algorithm \ref{alg:l2_2}).

\begin{algorithm}
    \caption{$SoftMax(\Vc{X})$ algorithm}
    \label{alg:l2_2}
    \begin{algorithmic}
      \STATE $\Vc{M} = P_{max}(\Vc{X})$
      \STATE $\Vc{T} = P_+(G_{exp}(\Vc{X})*G_{exp}(-\Vc{M}))$
      \STATE --------------------------------
      \STATE $\Vc{T} = \Vc{T}*\exp{\Vc{M}}$    \COMMENT{ \# Summary}
      \STATE --------------------------------
      \STATE $\Vc{X} = N_*(G_{exp}(\Vc{X}),\Vc{T})$ 
    \end{algorithmic}
\end{algorithm}


Let us go back using the original notations and block the input matrix
so that to exploit its natural composition, independent computations
by row and we exploit the associativity of the projection in Algorithm \ref{alg:l2_3}

\begin{algorithm}
    \caption{Blocked $3\times 3$}
    \label{alg:l2_3}
    \begin{algorithmic}
      \STATE \[
        \Vc{X}  =   \begin{pmatrix}
          \Vc{X}_{1,1} &    \Vc{X}_{1,2}&    \Vc{X}_{1,3}  \\
          \Vc{X}_{2,1} &    \Vc{X}_{2,2}&    \Vc{X}_{2,3}  \\
          \Vc{X}_{3,1} &    \Vc{X}_{3,2}&    \Vc{X}_{3,3}  \\
        \end{pmatrix}
      \] \\
      \WHILE{$i$ in [1,2,3] \# Independent } 
          \STATE $\Vc{T} = \Vc{0} $  \COMMENT{ \# Zero element for $P_+$}
          \WHILE {$j$ in [1, 2,3] \# Projection }
              \STATE $\Vc{T} = \Vc{T} +P_+(G_{f}(\Vc{X}_{i,j}))$ \COMMENT{   \# Reduction}  
          \ENDWHILE
          \STATE --------------------------------     
          \STATE $\Vc{T}  =   S_+(\Vc{T})$       \COMMENT{ \# Summary}
          \STATE --------------------------------     
          \WHILE {$j$ in [1,2,3] \# Normalization }
             \STATE $ \Vc{X}_{i,j} = N_*(\Vc{X}_{i,j},\Vc{T})$  
          \ENDWHILE
      \ENDWHILE
    \end{algorithmic}
\end{algorithm}

The application of the blocked algorithm for the Euclidean Norm and
for Softmax requires very little re-work, just remember that softmax
will require two projections:

\begin{equation} \Vc{M}_0 = -\infty; \Vc{T}_0 = 0 \end{equation}
\begin{equation} \Vc{M}_{j+1} = max(M_j, P_{max}(\Vc{X}_{i,j}) \end{equation}
\begin{equation} \Vc{T}_{j+1} = \Vc{T}_j*(exp(-(\Vc{M}_{j+1}-\Vc{M}_j))) + P_+(G_{\exp}(\Vc{X}_{i,j}-\Vc{M}_{j+1})) \end{equation}

But once the obstacle of recognizing the particular projection is
surmounted, the matching of the proper computation should be
natural. Also, the layer norm $(x_{i,j} - \mu_i)/\sqrt{\sigma^2_i}$
(without column scaling) will follow the softmax algorithm because it
will require the computation of two projections so that average
($\mu$) and variance ($\sigma^2$) can be computed in a single pass (Algorithm \ref{alg:l2_4}).

\begin{algorithm}
    \caption{Layer Norm $3\times 3$ with $\gamma, \beta$}
    \label{alg:l2_4}
    \begin{algorithmic}
      \STATE \[
        \Vc{X}  =   \begin{pmatrix}
          \Vc{X}_{1,1} &    \Vc{X}_{1,2}&    \Vc{X}_{1,3}  \\
          \Vc{X}_{2,1} &    \Vc{X}_{2,2}&    \Vc{X}_{2,3}  \\
          \Vc{X}_{3,1} &    \Vc{X}_{3,2}&    \Vc{X}_{3,3}  \\
        \end{pmatrix}
      \] 
      \STATE \[
        \Vc{\gamma}  =   \begin{pmatrix}
          \Vc{\gamma}_{1} &    \Vc{\gamma}_{2}&    \Vc{\gamma}_{3}  \\
        \end{pmatrix}
      \] 
      \STATE \[
        \Vc{\beta}  =   \begin{pmatrix}
          \Vc{\beta}_{1} &    \Vc{\beta}_{2}&    \Vc{\beta}_{3}  \\
        \end{pmatrix}
      \] 
      \WHILE{$i$ in [1,2,3] }
          \STATE $\Vc{T} = \Vc{0} $ 
          \STATE $\Vc{S} = \Vc{0} $ 
          \WHILE {$j$ in [1, 2,3] }
              \STATE $\Vc{T} = \Vc{T} +P_+(\Vc{X}_{i,j})$             \COMMENT{Projection + sum}  
              \STATE $\Vc{S} = \Vc{T} +P_+(G_{x*x}(\Vc{X}_{i,j}))$ \COMMENT{Projection + sum squares}  
          \ENDWHILE
          \STATE --------------------------------     
          \STATE $\Vc{T}  =   \Vc{T}/N$       \COMMENT{Summary, scalar}
          \STATE $\Vc{S}  =   (\Vc{S} - \Vc{T}^2/N)/N$       \COMMENT{Summary, scalar}
          \STATE --------------------------------     
          \WHILE {$j$ in [1,2,3] }
             \STATE $ \Vc{X}_{i,j} = \Vc{X}_{i,j}-\Vc{T}$  \COMMENT{Normalization Row}
             \STATE $ \Vc{X}_{i,j} = \Vc{X}_{i,j}/\Vc{S}$   
             \STATE $ \Vc{X}_{i,j} = \Vc{X}_{i,j}*\gamma_j$   \COMMENT{Normalization Column}
             \STATE $ \Vc{X}_{i,j} = \Vc{X}_{i,j} + \beta_j$   
          \ENDWHILE
      \ENDWHILE
    \end{algorithmic}
\end{algorithm}


\section{Computation Modes} 
The mathematical notation provides a clear description of the
computation, clearly pedantic but constructive in nature. Also, the
normalization factor and its computation can be used when we would
like to fuse the norm operation with the following (or the previous)
computation.

Here, we are more interested in describing how we would implement the
norm computation using different engine: GPU, GPU, and AIE FGPA
engines. We show direct implementations for CPU and AIE and we present
a short discussion about the GPUs. This is always related to AMD chips
and codes.

The main difference is about the computation of the Projection and how
the hardware implicitly helps the computation. Let's start with the
GPU that uses heavily the associativity for parallelization of the
computation.

\subsection{GPU conversation}
The projection is associative and we can exploit the parallelism by
blocking the computation in warps, compute independent and separated
$\Vc{T}_i = P_+(\Vc{X}_{i,j})$, then reduce the computation in cores
by using a tree like computation and built in HW utilities (register
computations). 

\begin{equation}
  \begin{matrix}
    \Vc{T}_1 = P_+(\Vc{X}_{i,1}), & ... &  \Vc{T}_n = P_+(\Vc{X}_{i,n}) \\ 
    \Vc{T}_1 = \Vc{T}_1+\Vc{T}_2, & ... &  \Vc{T}_{\frac{n}{2}} = \Vc{T}_{\frac{n}{2}} + \Vc{T}_{\frac{n}{2}+1} \\
    \Vc{T}_1 = \Vc{T}_1 + \Vc{T}_2, & ... &  \Vc{T}_{\frac{n}{4}} = \Vc{T}_{\frac{n}{4}} + \Vc{T}_{\frac{n}{4}+1} \\
    \dots  & ... & ... \\ 
    \Vc{T}_1 = \Vc{T}_1 + \Vc{T}_2 & & \\
  \end{matrix}
\end{equation}

The small size of the computation that can be done as unit and the HW
support for the reduction makes this computation
appealing. Numerically, the binary three computation make the
computation balanced and well partitioned. If there is no use of high
precision accumulators this will have numerical advantages. That is, a
balanced binary tree with $N$ elements has the longest addition path
length of only $K = \log_2(N)$ and the fish spine tree has $K = N-1$
and for additions we accumulate an error of $O(K)$. 

For a Layer Norm of size $512 \times 768$ on a MI100 with 1.2 TB/s
HBM2 we can achieve 9 us (microseconds) latency (bandwidth utilization
800GBs) for layer norm in fp16 and fp32 precision. This is the fastest
performance across the AMD products at our disposal.

\subsection{CPU conversation}
CPUs have deep memory hierachies to exploit temporal locality. If we
observe the norm computations there is little temporal locality. Each
row is read twice, but there will be $N$ accesses in between. As soon
as we realize that $\Vc{T} += P_+(\Vc{X}_{i,j})$ has consecutive
accecesses to fast L1 caches we can exploit the AVX instruction set to
transform scalar operations into parallel vector operations.

The partition on $\Vc{X}$ will associate a set of row to a
thread/core. We stream the input rows and we store in the core the
temporary $T$ (registers), we compute the factor(s), then we stream
the rows again for the propagation of the normalization factor(s). We
exploit temporal locality because the largest cache in the memory
hierarchy containing the rows will be read twice naturally without any
further assistance. When the computation moves to the next rows, we
clear the memory hierarch safely. C threads can deploy this strategy
naturally and without the need to much careful blocking. However,
threading is heavy and they are really applicable only for very large
matrices.

By writing the code so that the compiler can exploit AVX operation for
the projections and normalization and EPYC processor can achieve 147us
using a single core for $512 \times 768$.


\subsection{AIE Tiling generation and validation}
The AIE instruction set has vector operations per core (like GPU), a
relatively large data cache but it has poor inter core programmable
interconnectivity (often we have to use L2 and L3 for intercore communications).























\section{Conclusion}


  







%\input{conclusion.tex}

%%%%%%%%% -- BIB STYLE AND FILE -- %%%%%%%%
\bibliographystyle{IEEETran} \bibliography{ref}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\appendix{Review and Response}
%\input{review.tex}
\end{document}




  


  




  


